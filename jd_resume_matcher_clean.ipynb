{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dWIEcpT85wzG"
   },
   "outputs": [],
   "source": [
    "# installing the libraries that we need\n",
    "!pip install -q torch transformers sentence-transformers wandb scikit-learn rapidfuzz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VExIxTz3nCuN"
   },
   "outputs": [],
   "source": [
    "!pip install rapidfuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wcZHyDcf68JT"
   },
   "outputs": [],
   "source": [
    "# Standard Imports\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import textwrap\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "#Numpy/Pandas for data wrangling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#HF sentence-transformers for sentence/phrase embeddings\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "#Fuzzy string matching for messy text\n",
    "from rapidfuzz import fuzz, process\n",
    "\n",
    "#Using this to generate simple tables/reports in the notebook\n",
    "from IPython.display import display, HTML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L-HMrz0LBd0l"
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "wandb.init(\n",
    "    project=\"jd-resume-tracker\",\n",
    "    name=\"baseline-hf-embeddings\",\n",
    "    config={\n",
    "        \"embedding_model\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        \"skill_vocab_version\": \"v0.1\",\n",
    "        \"similarity_metric\": \"cosine\",\n",
    "        \"use_fuzzy_fallback\": True\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5qpOPGAejfUR"
   },
   "outputs": [],
   "source": [
    "#Example job description text(short)\n",
    "job_description_text=\"\"\"We are hiring a Machine Learning Engineer with strong experience in Python, PyTorch,\n",
    "TensorFlow, and AWS. You should know data pipelines (Airflow), SQL, Spark, Docker,\n",
    "Kubernetes, Git, and ML experiment tracking (Weights & Biases or MLflow).\n",
    "NLP experience with Hugging Face is a plus.\n",
    "\"\"\"\n",
    "\n",
    "#Example resume text(short)\n",
    "resume_text=\"\"\"Sophomore EE with CS/AI focus. Projects in NLP and Transformers using Hugging Face.\n",
    "Comfortable with Python, Pandas, NumPy, scikit-learn, TensorFlow. Basic PyTorch.\n",
    "Experience with Git/GitHub, Docker, SQL. Learning Airflow and Spark. Used Weights & Biases.\n",
    "Some AWS exposure.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bVIUbbTgj9Rp"
   },
   "outputs": [],
   "source": [
    "# A small practical skills vocabualry to start.\n",
    "skill_vocab=[\n",
    "     # Languages & Core\n",
    "    \"Python\", \"SQL\", \"Java\", \"C++\",\n",
    "    # ML/DL\n",
    "    \"PyTorch\", \"TensorFlow\", \"scikit-learn\", \"Hugging Face\", \"Transformers\",\n",
    "    # Data\n",
    "    \"Pandas\", \"NumPy\", \"Spark\", \"Airflow\",\n",
    "    # DevOps/Platform\n",
    "    \"Docker\", \"Kubernetes\", \"Git\", \"GitHub\",\n",
    "    # Tracking/Experimentation\n",
    "    \"Weights & Biases\", \"W&B\", \"MLflow\",\n",
    "    # Cloud\n",
    "    \"AWS\", \"SageMaker\", \"Lambda\", \"API Gateway\", \"S3\", \"Comprehend\", \"Bedrock\"\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GVQ85qRTlcSp"
   },
   "outputs": [],
   "source": [
    "# Correct model name\n",
    "embedding_model_name = wandb.config.get(\"embedding_model\", \"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Load the SentenceTransformer model\n",
    "embedder = SentenceTransformer(embedding_model_name)  # downloads from HF if not cached\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "WBhTEeO8puCe"
   },
   "outputs": [],
   "source": [
    "def normalize_text(t: str) -> str:\n",
    "    \"\"\"\n",
    "    Light normalization to reduce noise:\n",
    "    - lowercasing\n",
    "    - trim spaces\n",
    "    - collapse multiple spaces\n",
    "    \"\"\"\n",
    "    # Convert to lowercase for case-insensitive matching\n",
    "    t = t.lower()\n",
    "    # Strip leading/trailing whitespace\n",
    "    t = t.strip()\n",
    "    # Collapse multiple spaces/newlines into single spaces\n",
    "    t = re.sub(r\"\\s+\", \" \", t)\n",
    "    return t\n",
    "\n",
    "def extract_candidate_phrases(t: str, min_len=2, max_len=5) -> List[str]:\n",
    "    \"\"\"\n",
    "    Very simple phrase candidate extraction:\n",
    "    - Split text into tokens by non-letter chars\n",
    "    - Build n-grams (2 to 5 words) + unigrams that look like skills\n",
    "    This is a quick heuristic; we'll lean on embeddings + fuzzy later.\n",
    "    \"\"\"\n",
    "    # Replace non-letters/numbers with spaces and split\n",
    "    tokens = re.findall(r\"[a-zA-Z0-9\\+\\.\\-]+\", t)\n",
    "    # Collect unigrams that might be skills (e.g., 'python', 'pytorch')\n",
    "    unigrams = [tok for tok in tokens if len(tok) >= 2]\n",
    "    # Build simple n-grams (2..5 words) to catch multiword skills (e.g., 'hugging face')\n",
    "    ngrams = []\n",
    "    for n in range(min_len, max_len + 1):\n",
    "        for i in range(len(tokens) - n + 1):\n",
    "            ngrams.append(\" \".join(tokens[i:i+n]))\n",
    "    # Combine and deduplicate while keeping order\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for p in unigrams + ngrams:\n",
    "        if p not in seen:\n",
    "            seen.add(p)\n",
    "            out.append(p)\n",
    "    return out\n",
    "\n",
    "def embed_texts(texts: List[str]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute embeddings for a list of strings using the SentenceTransformer embedder.\n",
    "    Returns a NumPy array of shape (len(texts), dim).\n",
    "    \"\"\"\n",
    "    # Encode returns a list/array of vectors; convert to numpy for convenience\n",
    "    return np.array(embedder.encode(texts, normalize_embeddings=True))\n",
    "\n",
    "def top_matches(\n",
    "    queries: List[str],\n",
    "    corpus: List[str],\n",
    "    k: int = 5,\n",
    "    use_fuzzy: bool = True,\n",
    "    fuzzy_threshold: int = 90\n",
    ") -> Dict[str, List[Tuple[str, float]]]:\n",
    "    \"\"\"\n",
    "    For each query (candidate phrase), find top-k matches in the corpus (skill vocab).\n",
    "    - Primary: cosine similarity on embeddings.\n",
    "    - Fallback: fuzzy matching to catch spelling/format differences.\n",
    "    Returns dict: query -> list of (matched_skill, score) tuples.\n",
    "    \"\"\"\n",
    "    # Compute embeddings for both sides\n",
    "    q_emb = embed_texts(queries)\n",
    "    c_emb = embed_texts(corpus)\n",
    "\n",
    "    # Cosine similarity matrix (queries x corpus)\n",
    "    sim = np.matmul(q_emb, c_emb.T)  # normalized embeddings => dot product == cosine similarity\n",
    "\n",
    "    results = {}\n",
    "    for i, q in enumerate(queries):\n",
    "        # Get top-k by cosine\n",
    "        idxs = np.argsort(-sim[i])[:k]\n",
    "        pairs = [(corpus[j], float(sim[i, j])) for j in idxs]\n",
    "\n",
    "        # Optional fuzzy pass if best cosine is weak and we want to catch typos\n",
    "        if use_fuzzy and (len(pairs) == 0 or pairs[0][1] < 0.5):\n",
    "            # Use rapidfuzz to find likely literal matches (threshold adjustable)\n",
    "            fuzzy = process.extract(q, corpus, scorer=fuzz.token_sort_ratio, limit=k)\n",
    "            # Convert rapidfuzz (str, score, idx) -> (str, normalized_score)\n",
    "            fuzzy = [(name, score / 100.0) for name, score, _ in fuzzy if score >= fuzzy_threshold]\n",
    "            if fuzzy:\n",
    "                pairs = fuzzy\n",
    "\n",
    "        results[q] = pairs\n",
    "    return results\n",
    "\n",
    "def choose_skills(\n",
    "    phrase_to_matches: Dict[str, List[Tuple[str, float]]],\n",
    "    min_cosine: float = 0.6\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Convert phrase->matches into a final deduped skill list.\n",
    "    - Keep matches with cosine >= min_cosine (or fuzzy normalized score >= min_cosine).\n",
    "    - Deduplicate by best score.\n",
    "    \"\"\"\n",
    "    best: Dict[str, float] = {}\n",
    "    for phrase, matches in phrase_to_matches.items():\n",
    "        for skill, score in matches:\n",
    "            if score >= min_cosine:\n",
    "                if skill not in best or score > best[skill]:\n",
    "                    best[skill] = score\n",
    "    # Sort skills by score (desc) for presentation\n",
    "    return [s for s, _ in sorted(best.items(), key=lambda kv: -kv[1])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dMeERm0a5U_y"
   },
   "outputs": [],
   "source": [
    "# Normalize inputs for clean processing\n",
    "jd_norm = normalize_text(job_description_text)\n",
    "cv_norm = normalize_text(resume_text)\n",
    "\n",
    "# Extract crude candidate phrases from both\n",
    "jd_candidates = extract_candidate_phrases(jd_norm)\n",
    "cv_candidates = extract_candidate_phrases(cv_norm)\n",
    "\n",
    "# Compute matches from candidates to our curated skill vocabulary\n",
    "jd_phrase_matches = top_matches(jd_candidates, [s.lower() for s in skill_vocab], k=3)\n",
    "cv_phrase_matches = top_matches(cv_candidates, [s.lower() for s in skill_vocab], k=3)\n",
    "\n",
    "# Decide which vocab skills appear in each text\n",
    "jd_skills = choose_skills(jd_phrase_matches, min_cosine=0.6)\n",
    "cv_skills = choose_skills(cv_phrase_matches, min_cosine=0.6)\n",
    "\n",
    "# Convert back to canonical casing by mapping lower->original\n",
    "lower_to_original = {s.lower(): s for s in skill_vocab}\n",
    "jd_skills = [lower_to_original[s] for s in jd_skills if s in lower_to_original]\n",
    "cv_skills = [lower_to_original[s] for s in cv_skills if s in lower_to_original]\n",
    "\n",
    "# Match logic: overlap and gaps\n",
    "have = sorted(set(jd_skills).intersection(cv_skills))\n",
    "missing = sorted(set(jd_skills) - set(cv_skills))\n",
    "coverage = 0.0 if len(jd_skills) == 0 else len(have) / len(jd_skills)\n",
    "\n",
    "# Log to W&B so we can compare runs later\n",
    "wandb.log({\n",
    "    \"jd_skill_count\": len(jd_skills),\n",
    "    \"cv_skill_count\": len(cv_skills),\n",
    "    \"overlap_count\": len(have),\n",
    "    \"missing_count\": len(missing),\n",
    "    \"coverage\": coverage\n",
    "})\n",
    "\n",
    "# Display a quick report\n",
    "display(HTML(f\"\"\"\n",
    "<h3>Quick Match Report</h3>\n",
    "<p><b>JD skills:</b> {', '.join(jd_skills) if jd_skills else 'None'}</p>\n",
    "<p><b>Resume skills:</b> {', '.join(cv_skills) if cv_skills else 'None'}</p>\n",
    "<p><b>Overlap (you have):</b> {', '.join(have) if have else 'None'}</p>\n",
    "<p><b>Missing (to improve):</b> {', '.join(missing) if missing else 'None'}</p>\n",
    "<p><b>Match coverage:</b> {coverage:.2%}</p>\n",
    "\"\"\"))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
